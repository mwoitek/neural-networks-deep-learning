{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "Welcome to your week 4 assignment (part 1 of 2)! You have previously trained a\n",
    "2-layer Neural Network (with a single hidden layer). This week, you will build a\n",
    "deep neural network, with as many layers as you want!\n",
    "\n",
    "- In this notebook, you will implement all the functions required to build a\n",
    "deep neural network.\n",
    "- In the next assignment, you will use these functions to build a deep neural\n",
    "network for image classification.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Use non-linear units like ReLU to improve your model\n",
    "- Build a deeper neural network (with more than 1 hidden layer)\n",
    "- Implement an easy-to-use neural network class\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer.\n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and\n",
    "$b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example.\n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's\n",
    "activations).\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### <font color='darkblue'> Updates to Assignment <font>\n",
    "\n",
    "#### If you were working on a previous version\n",
    "* The current notebook filename is version \"4a\".\n",
    "* You can find your work in the file directory as version \"4\".\n",
    "* To see the file directory, click on the Coursera logo at the top left of the\n",
    "notebook.\n",
    "\n",
    "#### List of Updates\n",
    "* compute_cost unit test now includes tests for Y = 0 as well as Y = 1.  This\n",
    "catches a possible bug before students get graded.\n",
    "* linear_backward unit test now has a more complete unit test that catches a\n",
    "possible bug before students get graded.\n",
    "\n",
    "## 1 - Packages\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with\n",
    "Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- dnn_utils provides some necessary functions for this notebook.\n",
    "- testCases provides some test cases to assess the correctness of your functions\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. It\n",
    "will help us grade your work. Please don't change the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dnn_utils_v2 import relu, relu_backward, sigmoid, sigmoid_backward\n",
    "from testCases_v4a import *\n",
    "\n",
    "%matplotlib inline\n",
    "# Set default size of plots:\n",
    "plt.rcParams[\"figure.figsize\"] = (5.0, 4.0)\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Outline of the Assignment\n",
    "\n",
    "To build your neural network, you will be implementing several \"helper\n",
    "functions\". These helper functions will be used in the next assignment to build\n",
    "a two-layer neural network and an L-layer neural network. Each small helper\n",
    "function you will implement will have detailed instructions that will walk you\n",
    "through the necessary steps. Here is an outline of this assignment, you will:\n",
    "\n",
    "- Initialize the parameters for a two-layer network and for an $L$-layer neural\n",
    "network.\n",
    "- Implement the forward propagation module (shown in purple in the figure\n",
    "below).\n",
    "     - Complete the LINEAR part of a layer's forward propagation step (resulting\n",
    "in $Z^{[l]}$).\n",
    "     - We give you the ACTIVATION function (relu/sigmoid).\n",
    "     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward\n",
    "function.\n",
    "     - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through\n",
    "L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$). This\n",
    "gives you a new L_model_forward function.\n",
    "- Compute the loss.\n",
    "- Implement the backward propagation module (denoted in red in the figure\n",
    "below).\n",
    "    - Complete the LINEAR part of a layer's backward propagation step.\n",
    "    - We give you the gradient of the ACTIVATE function\n",
    "(relu_backward/sigmoid_backward)\n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward\n",
    "function.\n",
    "    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward\n",
    "in a new L_model_backward function\n",
    "- Finally update the parameters.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Note** that for every forward function, there is a corresponding backward\n",
    "function. That is why at every step of your forward module you will be storing\n",
    "some values in a cache. The cached values are useful for computing gradients. In\n",
    "the backpropagation module you will then use the cache to calculate the\n",
    "gradients. This assignment will show you exactly how to carry out each of these\n",
    "steps.\n",
    "\n",
    "## 3 - Initialization\n",
    "\n",
    "You will write two helper functions that will initialize the parameters for your\n",
    "model. The first function will be used to initialize parameters for a two layer\n",
    "model. The second one will generalize this initialization process to $L$ layers.\n",
    "\n",
    "### 3.1 - 2-layer Neural Network\n",
    "\n",
    "**Exercise**: Create and initialize the parameters of the 2-layer neural\n",
    "network.\n",
    "\n",
    "**Instructions**:\n",
    "- The model's structure is: *LINEAR -> RELU -> LINEAR -> SIGMOID*.\n",
    "- Use random initialization for the weight matrices. Use\n",
    "`np.random.randn(shape)*0.01` with the correct shape.\n",
    "- Use zero initialization for the biases. Use `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_x: Size of the input layer.\n",
    "    n_h: Size of the hidden layer.\n",
    "    n_y: Size of the output layer.\n",
    "\n",
    "    Returns:\n",
    "    params: Dictionary containing the following parameters:\n",
    "            W1: Weight matrix [Dimension: (n_h, n_x)];\n",
    "            b1: Bias vector [Dimension: (n_h, 1)];\n",
    "            W2: Weight matrix [Dimension: (n_y, n_h)];\n",
    "            b2: Bias vector [Dimension: (n_y, 1)].\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "\n",
    "    params = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(3, 2, 1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "### 3.2 - L-layer Neural Network\n",
    "\n",
    "The initialization for a deeper L-layer neural network is more complicated\n",
    "because there are many more weight matrices and bias vectors. When completing\n",
    "the `initialize_parameters_deep`, you should make sure that your dimensions\n",
    "match between each layer. Recall that $n^{[l]}$ is the number of units in layer\n",
    "$l$. Thus for example if the size of our input $X$ is $(12288, 209)$ (with\n",
    "$m=209$ examples) then:\n",
    "\n",
    "<table style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr>\n",
    "    <td>  </td>\n",
    "    <td> **Shape of W** </td>\n",
    "    <td> **Shape of b**  </td>\n",
    "    <td> **Activation** </td>\n",
    "    <td> **Shape of Activation** </td>\n",
    "<tr>\n",
    "\n",
    "<tr>\n",
    "    <td> **Layer 1** </td>\n",
    "    <td> $(n^{[1]},12288)$ </td>\n",
    "    <td> $(n^{[1]},1)$ </td>\n",
    "    <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>\n",
    "\n",
    "    <td> $(n^{[1]},209)$ </td>\n",
    "<tr>\n",
    "\n",
    "<tr>\n",
    "    <td> **Layer 2** </td>\n",
    "    <td> $(n^{[2]}, n^{[1]})$  </td>\n",
    "    <td> $(n^{[2]},1)$ </td>\n",
    "    <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td>\n",
    "    <td> $(n^{[2]}, 209)$ </td>\n",
    "<tr>\n",
    "\n",
    "   <tr>\n",
    "    <td> $\\vdots$ </td>\n",
    "    <td> $\\vdots$  </td>\n",
    "    <td> $\\vdots$  </td>\n",
    "    <td> $\\vdots$</td>\n",
    "    <td> $\\vdots$  </td>\n",
    "<tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "        <td> **Layer L-1** </td>\n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td>\n",
    "        <td> $(n^{[L-1]}, 1)$  </td>\n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td>\n",
    "        <td> $(n^{[L-1]}, 209)$ </td>\n",
    "    <tr>\n",
    "\n",
    "\n",
    "   <tr>\n",
    "        <td> **Layer L** </td>\n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td>\n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td>\n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Remember that when we compute $W X + b$ in python, it carries out broadcasting.\n",
    "For example, if:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r\n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i\n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$\n",
    "\n",
    "**Exercise**: Implement initialization for an L-layer Neural Network.\n",
    "\n",
    "**Instructions**:\n",
    "- The model's structure is *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR ->\n",
    "SIGMOID*. I.e., it has $L-1$ layers using a ReLU activation function followed by\n",
    "an output layer with a sigmoid activation function.\n",
    "- Use random initialization for the weight matrices. Use `np.random.randn(shape)\n",
    "* 0.01`.\n",
    "- Use zeros initialization for the biases. Use `np.zeros(shape)`.\n",
    "- We will store $n^{[l]}$, the number of units in different layers, in a\n",
    "variable `layer_dims`. For example, the `layer_dims` for the \"Planar Data\n",
    "classification model\" from last week would have been [2,4,1]: There were two\n",
    "inputs, one hidden layer with 4 hidden units, and an output layer with 1 output\n",
    "unit. This means `W1`'s shape was (4,2), `b1` was (4,1), `W2` was (1,4) and `b2`\n",
    "was (1,1). Now you will generalize this to $L$ layers!\n",
    "- Here is the implementation for $L=1$ (one layer neural network). It should\n",
    "inspire you to implement the general case (L-layer neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0])\n",
    "* 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims: List containing the dimension of each layer.\n",
    "\n",
    "    Returns:\n",
    "    parameters: Dictionary containing the parameters W1, b1, ..., WL, bL.\n",
    "                Wl: Weight matrix [Dimension: (layer_dims[l], layer_dims[l - 1])].\n",
    "                bl: Bias vector [Dimension: (layer_dims[l], 1)].\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    # Number of layers:\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "\n",
    "        temp1 = str(l)\n",
    "        temp2 = \"b\" + temp1\n",
    "        temp1 = \"W\" + temp1\n",
    "        temp3 = layer_dims[l]\n",
    "        temp4 = layer_dims[l - 1]\n",
    "\n",
    "        parameters[temp1] = np.random.randn(temp3, temp4) * 0.01\n",
    "        parameters[temp2] = np.zeros((temp3, 1))\n",
    "\n",
    "        assert(parameters[temp1].shape == (temp3, temp4))\n",
    "        assert(parameters[temp2].shape == (temp3, 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep([5, 4, 3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "## 4 - Forward propagation module\n",
    "\n",
    "### 4.1 - Linear Forward\n",
    "Now that you have initialized your parameters, you will do the forward\n",
    "propagation module. You will start by implementing some basic functions that you\n",
    "will use later when implementing the model. You will complete three functions in\n",
    "this order:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the\n",
    "following equations:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $A^{[0]} = X$.\n",
    "\n",
    "**Exercise**: Build the linear part of forward propagation.\n",
    "\n",
    "**Reminder**:\n",
    "The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]}\n",
    "+b^{[l]}$. You may also find `np.dot()` useful. If your dimensions don't match,\n",
    "printing `W.shape` may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A: Activations from the previous layer;\n",
    "       Dimension: (size of previous layer, number of examples).\n",
    "    W: Weight matrix;\n",
    "       Dimension: (size of current layer, size of previous layer).\n",
    "    b: Bias vector;\n",
    "       Dimension: (size of current layer, 1).\n",
    "\n",
    "    Returns:\n",
    "    Z: Input of the activation function (pre-activation parameter).\n",
    "    cache: Tuple containing A, W and b; Stored for the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.matmul(W, A) + b\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "### 4.2 - Linear-Activation Forward\n",
    "\n",
    "In this notebook, you will use two activation functions:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. We\n",
    "have provided you with the `sigmoid` function. This function returns **two**\n",
    "items: the activation value \"`a`\" and a \"`cache`\" that contains \"`Z`\" (it's what\n",
    "we will feed in to the corresponding backward function). To use it you could\n",
    "just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, activation_cache = sigmoid(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ReLU**: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$. We\n",
    "have provided you with the `relu` function. This function returns **two** items:\n",
    "the activation value \"`A`\" and a \"`cache`\" that contains \"`Z`\" (it's what we\n",
    "will feed in to the corresponding backward function). To use it you could just\n",
    "call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, activation_cache = relu(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more convenience, you are going to group two functions (Linear and\n",
    "Activation) into one function (LINEAR->ACTIVATION). Hence, you will implement a\n",
    "function that does the LINEAR forward step followed by an ACTIVATION forward\n",
    "step.\n",
    "\n",
    "**Exercise**: Implement the forward propagation of the *LINEAR->ACTIVATION*\n",
    "layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]}\n",
    "+b^{[l]})$ where the activation \"g\" can be sigmoid() or relu(). Use\n",
    "linear_forward() and the correct activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation for the LINEAR -> ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev: Activations from the previous layer;\n",
    "       Dimension: (size of previous layer, number of examples).\n",
    "    W: Weight matrix;\n",
    "       Dimension: (size of current layer, size of previous layer).\n",
    "    b: Bias vector;\n",
    "       Dimension: (size of current layer, 1).\n",
    "    activation: String containing the activation function to be used in this\n",
    "                layer; Options: \"relu\" or \"sigmoid\".\n",
    "\n",
    "    Returns:\n",
    "    A: Output of the activation function (post-activation value).\n",
    "    cache: Tuple containing linear_cache and activation_cache; Stored for the\n",
    "           backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(\n",
    "    A_prev,\n",
    "    W,\n",
    "    b,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(\n",
    "    A_prev,\n",
    "    W,\n",
    "    b,\n",
    "    activation=\"relu\"\n",
    ")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Note**: In deep learning, the \"[LINEAR->ACTIVATION]\" computation is counted as\n",
    "a single layer in the neural network, not two layers.\n",
    "\n",
    "### d) L-Layer Model\n",
    "\n",
    "For even more convenience when implementing the $L$-layer Neural Net, you will\n",
    "need a function that replicates the previous one (`linear_activation_forward`\n",
    "with RELU) $L-1$ times, then follows that with one `linear_activation_forward`\n",
    "with SIGMOID.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\"\n",
    "style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR ->\n",
    "SIGMOID* model</center></caption><br>\n",
    "\n",
    "**Exercise**: Implement the forward propagation of the above model.\n",
    "\n",
    "**Instruction**: In the code below, the variable `AL` will denote $A^{[L]} =\n",
    "\\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also\n",
    "called `Yhat`, i.e., this is $\\hat{Y}$.)\n",
    "\n",
    "**Tips**:\n",
    "- Use the functions you had previously written\n",
    "- Use a for loop to replicate [LINEAR->RELU] (L-1) times\n",
    "- Don't forget to keep track of the caches in the \"caches\" list. To add a new\n",
    "value `c` to a `list`, you can use `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation for [LINEAR -> RELU] * (L - 1) -> LINEAR -> SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X: Input data [Dimension: (input size, number of examples)].\n",
    "    parameters: Output of initialize_parameters_deep.\n",
    "\n",
    "    Returns:\n",
    "    AL: Last post-activation value.\n",
    "    caches: List containing every cache of linear_activation_forward.\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    # Number of layers:\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    # [LINEAR -> RELU] * (L - 1):\n",
    "    for l in range(1, L):\n",
    "\n",
    "        temp1 = str(l)\n",
    "        temp2 = \"b\" + temp1\n",
    "        temp1 = \"W\" + temp1\n",
    "\n",
    "        A_prev = A\n",
    "        W = parameters[temp1]\n",
    "        b = parameters[temp2]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "\n",
    "        # Add cache to caches:\n",
    "        caches.append(cache)\n",
    "\n",
    "    # LINEAR -> SIGMOID:\n",
    "\n",
    "    temp1 = str(L)\n",
    "    temp2 = \"b\" + temp1\n",
    "    temp1 = \"W\" + temp1\n",
    "\n",
    "    A_prev = A\n",
    "    W = parameters[temp1]\n",
    "    b = parameters[temp2]\n",
    "    AL, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "\n",
    "    # Add cache to caches:\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Great! Now you have a full forward propagation that takes the input X and\n",
    "outputs a row vector $A^{[L]}$ containing your predictions. It also records all\n",
    "intermediate values in \"caches\". Using $A^{[L]}$, you can compute the cost of\n",
    "your predictions.\n",
    "\n",
    "## 5 - Cost function\n",
    "\n",
    "Now you will implement forward and backward propagation. You need to compute the\n",
    "cost, because you want to check if your model is actually learning.\n",
    "\n",
    "**Exercise**: Compute the cross-entropy cost $J$, using the following formula:\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) +\n",
    "(1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy cost function.\n",
    "\n",
    "    Arguments:\n",
    "    AL: Last post-activation value.\n",
    "    Y: Labels vector.\n",
    "\n",
    "    Returns:\n",
    "    cost: Cross-entropy cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the cost:\n",
    "    cost = np.mean(Y * np.log(AL))\n",
    "    cost += np.mean((1 - Y) * np.log(1 - AL))\n",
    "    cost *= -1\n",
    "\n",
    "    # Make sure cost is exactly what's expected:\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr>\n",
    "<td>**cost** </td>\n",
    "<td> 0.2797765635793422</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "Just like with forward propagation, you will implement helper functions for\n",
    "backpropagation. Remember that back propagation is used to calculate the\n",
    "gradient of the loss function with respect to the parameters.\n",
    "\n",
    "**Reminder**:\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figure 3** : Forward and Backward propagation for\n",
    "*LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward\n",
    "propagation, and the red blocks represent the backward propagation.*\n",
    "</center></caption>\n",
    "\n",
    "<!--\n",
    "For those of you who are expert in calculus (you don't need to be to do this\n",
    "assignment), the chain rule of calculus can be used to derive the derivative of\n",
    "the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as\n",
    "follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} =\n",
    "\\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}}\n",
    "\\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial\n",
    "W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]}\n",
    "\\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation,\n",
    "at each step you multiply your current gradient by the gradient corresponding to\n",
    "the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial\n",
    "L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} =\n",
    "dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "Now, similar to forward propagation, you are going to build the backward\n",
    "propagation in three steps:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of\n",
    "either the ReLU or sigmoid activation\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)\n",
    "\n",
    "### 6.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "(followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial\n",
    "\\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]},\n",
    "dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the\n",
    "input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m}\n",
    "dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m}\n",
    "\\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T}\n",
    "dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "**Exercise**: Use the 3 formulas above to implement linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linear part of backward propagation for a single layer.\n",
    "\n",
    "    Arguments:\n",
    "    dZ: Gradient of the cost with respect to the linear output of the current\n",
    "        layer.\n",
    "    cache: Tuple (A_prev, W, b) coming from the forward propagation in the\n",
    "           current layer.\n",
    "\n",
    "    Returns:\n",
    "    dA_prev: Gradient of the cost with respect to the activation of the\n",
    "             previous layer (same shape as A_prev).\n",
    "    dW: Gradient of the cost with respect to W (current layer); Same shape as W.\n",
    "    db: Gradient of the cost with respect to b (current layer); Same shape as b.\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    inv_m = 1 / m\n",
    "\n",
    "    dW = inv_m * np.matmul(dZ, np.transpose(A_prev))\n",
    "    db = inv_m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.matmul(np.transpose(W), dZ)\n",
    "\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"dA_prev = \" + str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_prev =\n",
    " [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "dW =\n",
    " [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
    " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
    " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
    "db =\n",
    " [[-0.14713786]\n",
    " [-0.11313155]\n",
    " [-0.13209101]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "Next, you will create a function that merges the two helper functions:\n",
    "**`linear_backward`** and the backward step for the activation\n",
    "**`linear_activation_backward`**.\n",
    "\n",
    "To help you implement `linear_activation_backward`, we provided two backward\n",
    "functions:\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit.\n",
    "You can call it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ = sigmoid_backward(dA, activation_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. You\n",
    "can call it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ = relu_backward(dA, activation_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $g(.)$ is the activation function,\n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} *\n",
    "g'(Z^{[l]}) \\tag{11}$$.\n",
    "\n",
    "**Exercise**: Implement the backpropagation for the *LINEAR->ACTIVATION* layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Backward propagation for the LINEAR -> ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA: Post-activation gradient (current layer).\n",
    "    cache: Tuple containing linear_cache and activation_cache.\n",
    "    activation: String containing the activation function to be used in this\n",
    "                layer; Options: \"relu\" or \"sigmoid\".\n",
    "\n",
    "    Returns:\n",
    "    dA_prev: Gradient of the cost with respect to the activation of the\n",
    "             previous layer (same shape as A_prev).\n",
    "    dW: Gradient of the cost with respect to W (current layer); Same shape as W.\n",
    "    db: Gradient of the cost with respect to b (current layer); Same shape as b.\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(\n",
    "    dAL,\n",
    "    linear_activation_cache,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "print(\"sigmoid:\")\n",
    "print(\"dA_prev = \" + str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(\n",
    "    dAL,\n",
    "    linear_activation_cache,\n",
    "    activation=\"relu\"\n",
    ")\n",
    "print(\"relu:\")\n",
    "print(\"dA_prev = \" + str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td>\n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td>\n",
    "\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr>\n",
    "<td > dW </td>\n",
    "       <td > [[ 0.10266786  0.09778551 -0.01968084]] </td>\n",
    "  </tr>\n",
    "\n",
    "<tr>\n",
    "<td > db </td>\n",
    "       <td > [[-0.05729622]] </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td>\n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td>\n",
    "\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr>\n",
    "<td > dW </td>\n",
    "       <td > [[ 0.44513824  0.37371418 -0.10478989]] </td>\n",
    "  </tr>\n",
    "\n",
    "<tr>\n",
    "<td > db </td>\n",
    "       <td > [[-0.20837892]] </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward\n",
    "\n",
    "Now you will implement the backward function for the whole network. Recall that\n",
    "when you implemented the `L_model_forward` function, at each iteration, you\n",
    "stored a cache which contains (X,W,b, and z). In the back propagation module,\n",
    "you will use those variables to compute the gradients. Therefore, in the\n",
    "`L_model_backward` function, you will iterate through all the hidden layers\n",
    "backward, starting from layer $L$. On each step, you will use the cached values\n",
    "for layer $l$ to backpropagate through layer $l$. Figure 5 below shows the\n",
    "backward pass.\n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Initializing backpropagation**:\n",
    "To backpropagate through this network, we know that the output is,\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "To do so, use this formula (derived using calculus which you don't need in-depth\n",
    "knowledge of):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with\n",
    "respect to AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use this post-activation gradient `dAL` to keep going backward. As\n",
    "seen in Figure 5, you can now feed in `dAL` into the LINEAR->SIGMOID backward\n",
    "function you implemented (which will use the cached values stored by the\n",
    "L_model_forward function). After that, you will have to use a `for` loop to\n",
    "iterate through all the other layers using the LINEAR->RELU backward function.\n",
    "You should store each dA, dW, and db in the grads dictionary. To do so, use this\n",
    "formula :\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`.\n",
    "\n",
    "**Exercise**: Implement backpropagation for the *[LINEAR->RELU] $\\times$ (L-1)\n",
    "-> LINEAR -> SIGMOID* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Backward propagation for [LINEAR -> RELU] * (L - 1) -> LINEAR -> SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    AL: Last post-activation value.\n",
    "    Y: Labels vector.\n",
    "    caches: List containing every cache of linear_activation_forward.\n",
    "\n",
    "    Returns:\n",
    "    grads: Dictionary containing the gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # Number of layers:\n",
    "    L = len(caches)\n",
    "\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    # After this line, Y has the same shape as AL:\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    # Initialize backward propagation:\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # L-th layer (SIGMOID -> LINEAR) gradients:\n",
    "\n",
    "    current_cache = caches[L - 1]\n",
    "\n",
    "    str_1 = str(L)\n",
    "    str_2 = str(L - 1)\n",
    "    key_dA = \"dA\" + str_2\n",
    "    key_dW = \"dW\" + str_1\n",
    "    key_db = \"db\" + str_1\n",
    "\n",
    "    grads[key_dA], grads[key_dW], grads[key_db] = linear_activation_backward(\n",
    "        dAL,\n",
    "        current_cache,\n",
    "        \"sigmoid\"\n",
    "    )\n",
    "\n",
    "    # Loop through the remaining layers (l = L - 2 to l = 0):\n",
    "    for l in reversed(range(L - 1)):\n",
    "\n",
    "        # l-th layer (RELU -> LINEAR) gradients:\n",
    "\n",
    "        current_cache = caches[l]\n",
    "\n",
    "        str_1 = str(l + 1)\n",
    "        str_2 = str(l)\n",
    "        key_dA = \"dA\" + str_2\n",
    "        key_dW = \"dW\" + str_1\n",
    "        key_db = \"db\" + str_1\n",
    "\n",
    "        grads[key_dA], grads[key_dW], grads[key_db] = linear_activation_backward(\n",
    "            grads[\"dA\" + str_1],\n",
    "            current_cache,\n",
    "            \"relu\"\n",
    "        )\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "\n",
    "  <tr>\n",
    "    <td > dW1 </td>\n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr>\n",
    "<td > db1 </td>\n",
    "       <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "  <td > dA1 </td>\n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### 6.4 - Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient\n",
    "descent:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters,\n",
    "store them in the parameters dictionary.\n",
    "\n",
    "**Exercise**: Implement `update_parameters()` to update your parameters using\n",
    "gradient descent.\n",
    "\n",
    "**Instructions**:\n",
    "Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l\n",
    "= 1, 2, ..., L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters: Dictionary containing parameters.\n",
    "    grads: Dictionary containing gradients (output of L_model_backward).\n",
    "\n",
    "    Returns:\n",
    "    parameters: Dictionary containing the updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of layers:\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    # Update rule for each parameter:\n",
    "    for l in range(L):\n",
    "        temp = str(l + 1)\n",
    "        parameters[\"W\" + temp] -= learning_rate * grads[\"dW\" + temp]\n",
    "        parameters[\"b\" + temp] -= learning_rate * grads[\"db\" + temp]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "    <td > W1 </td>\n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td>\n",
    "  </tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr>\n",
    "<td > b1 </td>\n",
    "       <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "<td > W2 </td>\n",
    "       <td > [[-0.55569196  0.0354055   1.32964895]]</td>\n",
    "  </tr>\n",
    "\n",
    "<tr>\n",
    "<td > b2 </td>\n",
    "       <td > [[-0.84610769]] </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Conclusion\n",
    "\n",
    "Congrats on implementing all the functions required for building a deep neural\n",
    "network!\n",
    "\n",
    "We know it was a long assignment but going forward it will only get better. The\n",
    "next part of the assignment is easier.\n",
    "\n",
    "In the next assignment you will put all these together to build two models:\n",
    "- A two-layer neural network\n",
    "- An L-layer neural network\n",
    "\n",
    "You will in fact use these models to classify cat vs non-cat images!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
